{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This notebook involves some processing of the data from the openalex climate change dataset. It then filters the needed nodes for the graph database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import json_lines\n",
    "import fasttext\n",
    "from unidecode import unidecode\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect import LangDetectException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../data/open_alex/OA_2000.json\", \"r\") as f:\n",
    "    oa_2000 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "for w in oa_2000:\n",
    "    print(f\"{type(w)}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'doi', 'title', 'display_name', 'publication_year', 'publication_date', 'ids', 'language', 'primary_location', 'type', 'open_access', 'authorships', 'corresponding_author_ids', 'corresponding_institution_ids', 'apc_payment', 'cited_by_count', 'biblio', 'is_retracted', 'is_paratext', 'concepts', 'mesh', 'locations', 'best_oa_location', 'grants', 'referenced_works', 'related_works', 'ngrams_url', 'abstract_inverted_index', 'cited_by_api_url', 'counts_by_year', 'updated_date', 'created_date'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadOAJson:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def get_data(self):\n",
    "        fulldata = []\n",
    "        with open(self.filename) as f:\n",
    "            datalines = [l for l in json_lines.reader(f)]\n",
    "            jsonlen = len(\n",
    "                datalines[0]\n",
    "            )  # since each file contains several jsonlines, its looped\n",
    "            for idx in range(jsonlen):\n",
    "                fulldata.extend(datalines[0][idx])\n",
    "            datadf = pd.DataFrame(fulldata)\n",
    "            return datadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "foldername = \"../data/OpenAlex_Climate_Change/\"\n",
    "allfiles = os.listdir(foldername)\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for eachfile in allfiles:\n",
    "    print(\"Processing . . \", eachfile)\n",
    "    filename = ReadOAJson(foldername + eachfile)\n",
    "    data = filename.get_data()\n",
    "    df = df.append(data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fasttext model is used to detect which language the abstract is written in. I have already tried out langdetect, langid. So far fasttext is the fastest one. But it needs the pretrained model to be loaded and it could be found from [lid.176.bin(126mb)](https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmodel = fasttext.load_model(\"lid.176.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abstract(abs_idx):\n",
    "    if abs_idx:\n",
    "        words = [(kv[0], v) for kv in abs_idx.items() for v in kv[1]]\n",
    "        para = \" \".join([w[0] for w in sorted(words, key=lambda x: x[1])])\n",
    "        para = \". \".join([p for p in para.split(\".\")])\n",
    "        # the len is to filter those abstracts with just punctuations\n",
    "        if len(para.translate(str.maketrans(\"\", \"\", string.punctuation)).strip()) > 5:\n",
    "            # the fasttext model predicts only without new lines and [-2:] is to extract only the language id like en, de, it\n",
    "            return para, fmodel.predict(para.replace(\"\\n\", \"\"))[0][0][-2:]\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"PublicationAbstract\"], df[\"Language\"] = zip(\n",
    "    *df[\"abstract_inverted_index\"].apply(get_abstract)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Author\"] = df[\"authorships\"].apply(\n",
    "    lambda x: x[0][\"author\"][\"display_name\"] if x else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Author\"] = df[\"Author\"].apply(lambda x: unidecode(x) if x else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"OpenAlexConcept\"] = df[\"concepts\"].apply(lambda x: [i[\"display_name\"] for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Publication\"] = df[\"host_venue\"].apply(lambda x: x[\"publisher\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Venue\"] = df[\"host_venue\"].apply(lambda x: x[\"display_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Title\"] = df[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"timestamp\"] = df[\"publication_date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "has_abstract: bool = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [\n",
    "    \"timestamp\",\n",
    "    \"Title\",\n",
    "    \"Author\",\n",
    "    \"OpenAlexConcept\",\n",
    "    \"PublicationAbstract\",\n",
    "    \"Publication\",\n",
    "    \"Language\",\n",
    "    \"Venue\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodes = df[nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodes.to_json(\"../data/generated/openalex_climate_nodes.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading back from the file\n",
    "df = pd.read_json(\"../data/generated/openalex_climate_nodes.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_abstract: bool = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oa_tool.downloader import OpenAlexDownloader\n",
    "\n",
    "OpenAlexDownloader(\n",
    "    1950,\n",
    "    1952,\n",
    "    concepts=[\"C132651083\"],\n",
    "    email=\"roxanne.elbaff@dlr.de\",\n",
    "    has_abstract=True,\n",
    ").build_request_str(1900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstractInvertedIndex = {\n",
    "    \"Abstract\": [0],\n",
    "    \"An\": [1],\n",
    "    \"extended\": [2],\n",
    "    \"period\": [3],\n",
    "    \"numerical\": [4],\n",
    "    \"integration\": [5],\n",
    "    \"of\": [6, 21, 24, 79, 97, 118, 138],\n",
    "    \"a\": [7, 62, 71, 77, 95, 114],\n",
    "    \"baroclinic\": [8],\n",
    "    \"primitive\": [9],\n",
    "    \"equation\": [10],\n",
    "    \"model\": [11],\n",
    "    \"has\": [12],\n",
    "    \"been\": [13],\n",
    "    \"made\": [14],\n",
    "    \"for\": [15, 104],\n",
    "    \"the\": [16, 19, 22, 25, 40, 54, 66, 84, 88, 98, 102, 107, 129],\n",
    "    \"simulation\": [17],\n",
    "    \"and\": [18, 61, 87, 136, 147],\n",
    "    \"study\": [20],\n",
    "    \"dynamics\": [23],\n",
    "    \"atmosphere's\": [26],\n",
    "    \"general\": [27],\n",
    "    \"circulation.\": [28],\n",
    "    \"The\": [29, 47, 73, 116, 151],\n",
    "    \"solution\": [30],\n",
    "    \"corresponding\": [31, 82],\n",
    "    \"to\": [32, 44, 83, 93, 142],\n",
    "    \"external\": [33, 134],\n",
    "    \"gravitational\": [34],\n",
    "    \"propagation\": [35],\n",
    "    \"is\": [36, 76, 91, 111, 120],\n",
    "    \"filtered\": [37],\n",
    "    \"by\": [38],\n",
    "    \"requiring\": [39],\n",
    "    \"vertically\": [41],\n",
    "    \"integrated\": [42],\n",
    "    \"divergence\": [43],\n",
    "    \"vanish\": [45],\n",
    "    \"identically.\": [46],\n",
    "    \"vertical\": [48],\n",
    "    \"structure\": [49],\n",
    "    \"permits\": [50],\n",
    "    \"as\": [51, 70, 113, 125],\n",
    "    \"dependent\": [52],\n",
    "    \"variables\": [53],\n",
    "    \"horizontal\": [55],\n",
    "    \"wind\": [56],\n",
    "    \"at\": [57],\n",
    "    \"two\": [58],\n",
    "    \"internal\": [59],\n",
    "    \"levels\": [60],\n",
    "    \"single\": [63],\n",
    "    \"temperature,\": [64],\n",
    "    \"with\": [65],\n",
    "    \"static\": [67, 130],\n",
    "    \"stability\": [68],\n",
    "    \"entering\": [69],\n",
    "    \"parameter.\": [72, 115],\n",
    "    \"incoming\": [74],\n",
    "    \"radiation\": [75, 90],\n",
    "    \"function\": [78, 96],\n",
    "    \"latitude\": [80],\n",
    "    \"only\": [81, 124],\n",
    "    \"annual\": [85],\n",
    "    \"mean,\": [86],\n",
    "    \"outgoing\": [89],\n",
    "    \"taken\": [92, 121],\n",
    "    \"be\": [94],\n",
    "    \"local\": [99],\n",
    "    \"temperature.\": [100],\n",
    "    \"With\": [101],\n",
    "    \"requirement\": [103],\n",
    "    \"thermal\": [105],\n",
    "    \"equilibrium,\": [106],\n",
    "    \"domain\": [108],\n",
    "    \"mean\": [109],\n",
    "    \"temperature\": [110],\n",
    "    \"specified\": [112],\n",
    "    \"role\": [117],\n",
    "    \"condensation\": [119],\n",
    "    \"into\": [122],\n",
    "    \"account\": [123],\n",
    "    \"it\": [126],\n",
    "    \"effectively\": [127],\n",
    "    \"reduces\": [128],\n",
    "    \"stability.\": [131],\n",
    "    \"All\": [132],\n",
    "    \"other\": [133, 145],\n",
    "    \"sources\": [135],\n",
    "    \"sinks\": [137],\n",
    "    \"heat\": [139],\n",
    "    \"are\": [140, 148, 153],\n",
    "    \"assumed\": [141],\n",
    "    \"balance\": [143],\n",
    "    \"each\": [144],\n",
    "    \"locally,\": [146],\n",
    "    \"thus\": [149],\n",
    "    \"omitted.\": [150],\n",
    "    \"kinematics\": [152],\n",
    "    \"th...\": [154],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_inverted_to_text(abstracted_inverted_index):\n",
    "    word_index = []\n",
    "    for k, v in abstracted_inverted_index.items():\n",
    "        for index in v:\n",
    "            word_index.append([k, index])\n",
    "    word_index = sorted(word_index, key=lambda x: x[1])\n",
    "    abstract = \" \".join([x[0] for x in word_index])\n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the JSON data\n",
    "with open(\"oa_climate_change.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    print(data.keys())\n",
    "    print(data[\"meta\"])\n",
    "    works = []\n",
    "    for res in data[\"results\"]:\n",
    "        if (\n",
    "            \"abstract_inverted_index\" in res.keys()\n",
    "            and res[\"abstract_inverted_index\"] is not None\n",
    "        ):\n",
    "            res[\"abstract_text\"] = from_inverted_to_text(res[\"abstract_inverted_index\"])\n",
    "            del res[\"abstract_inverted_index\"]\n",
    "        works.append(res)\n",
    "\n",
    "\n",
    "# Normalize the JSON data to a Pandas DataFrame\n",
    "df = pd.json_normalize(works, max_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
